---
title: "Description of Approach"
author: "Martin Roth and Andrea Pagani"
date: "September 29, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, Libraries, include=FALSE}
library(data.table)
library(imager)
library(visDec)
library(ggplot2)
library(rpart)
library(rattle)
library(ROCR)
library(caret)
```

```{r, ExamplePictures, include=FALSE}
path <- system.file("extdata/Meetterrein", package="visDec")
filenames <- list.files(path,
                        pattern=glob2rx("Meetterrein_201510*.jpg"),
                        full.names=TRUE)
```

## Global feature approach

One problem with the presented landmark discrimination and contrast reduction
approaches is, that they require specific targets to be selected. An approach
to overcome this by focusing on image features is presented in the
following. First, we present the features we looked at so far and then we
present two different ways to use these features.

### Mean edges
Edge detection was used already in the landmark  discrimination approach. 
Instead of focusing on specific targets, we calculate now the mean number of 
edges in a given picture. Under daylight conditions this number can be viewed as
a relative indication of the fogginess. 

```{r, meanEdgeRegression, cache=TRUE, echo=FALSE, warning=FALSE, fig.height=4, fig.width=4}
load("results/deBiltResults2015.RData")
ggplot(imageSummary, aes(x = log(MOR), y = meanEdge)) + geom_point() 
```

```{r, Edges, cache=TRUE, include=FALSE, fig.height=4, fig.width=8.4}
imClear <-  subim(load.image(filenames[75]), y > 16)
#```{r, EdgesPlotClear, echo=FALSE, fig.cap=paste("Edges in a clear situation"), fig.height=4, fig.width=8.4}
#im <- subim(load.image(filenames[75]), y > 16)
#old_par <- par(mfrow=c(1,2))
#plot(im)
#DetectEdges(im) %>% plot
#par(old_par)
#```
#In a clear situation we observe `r round(DetectMeanEdges(im)*100,2)`% edges.
#
#```{r, EdgesPlotFoggy, echo=FALSE, fig.cap=paste("Edges in a foggy situation"), fig.height=4, fig.width=8.4}
#im <- subim(load.image(filenames[49]), y > 16)
#old_par <- par(mfrow=c(1,2))
#plot(im)
#DetectEdges(im) %>% plot
#par(old_par)
#```
imFoggy <- subim(load.image(filenames[49]), y > 16)
old_par <- par(mfrow=c(1,2))
plot(imClear)
plot(imFoggy)
par(old_par)
```

In the clear situation we have a fraction of 
`r round(DetectMeanEdges(imClear)*100,2)`% edges and in in the foggy situation
we observe `r round(DetectMeanEdges(imFoggy)*100,2)`% edges.

Note that in under night conditions fog actually results in relatively more
edges, because of the scattering of light sources by the water particles.  

### Transmission estimate using the Dark Channel prior

He et al. (2008) present an approach for haze removal from (single) images based
on the so-called dark channel prior. The dark channel prior is based on the
observation, "that most patches (small subimages) in a haze-free outdoor image
contain some pixels which have very low intensities in at least one color
channel". The idea was to follow this approach but instead of focusing on the
haze removal, we focus on the haze instead. Therefore, we estimate the
transmission in the local patches with the technique developed by 
He et al. (2008). From the transmission we estimate horizontal averages to 
infer the transition between sky and non-sky regions. In a clear situation this 
transition is relatively sharp. However, in a foggy situation the transition 
is less clear.

```{r, TransmissionPlotClear, cache = TRUE, echo=FALSE, fig.cap="Transmission in a clear situation", fig.height=3, fig.width=8.4, dependson='ExamplePictures'}
im <- subim(load.image(filenames[75]), y > 16)

darkChannel       <- GetDarkChannel(im)
atmosphere        <- GetAtmosphere(im, darkChannel)
transmissionClear <- GetTransmissionEstimate(im, atmosphere)

old_par <- par(mfrow=c(1,3))
plot(im)
as.cimg(transmissionClear) %>% plot
plot(GetHorizAvgTrans(im), xlab="", ylab="")
par(old_par)
```

```{r, TransmissionPlotFoggy, cache=TRUE, echo=FALSE, fig.cap="Transmission in a foggy situation", fig.height=3, fig.width=8.4, dependson='ExamplePictures'}
im <- subim(load.image(filenames[49]), y > 16)

darkChannel       <- GetDarkChannel(im)
atmosphere        <- GetAtmosphere(im, darkChannel)
transmissionFoggy <- GetTransmissionEstimate(im, atmosphere)

old_par <- par(mfrow=c(1,3))
plot(im)
as.cimg(transmissionFoggy) %>% plot()
plot(GetHorizAvgTrans(im), xlab="", ylab="")
par(old_par)
```

The idea is now to use the horizontal averages and compute the changepoint or a
measure of the variation of the curve and use these as features of the image.

It is clear that different features could be used as well. We looked for 
instance also at different color spaces and in the following we use also the
mean brightness of the image as a feature. 

### Classification
Using these (and possibly other) features we want to
develop a classification (and finally a clustering) scheme, to predict whether a
image is foggy or not. As training set we have data for the measurement site at
De Bilt, from June 1 2015 until December 31 2015 (16001 images). 
The training set are the images for the same fixed camera for the period January
1 till June 30 2016 (13883 images).

```{r, WindData, cache=TRUE}
source("R/ReadMeteoData.R")
windDeBilt <- ReadWindData("inst/extdata/Sensor/DeBiltWind1-1-2015-31-08-2016.csv")
#windTwente <- ReadWindData()
setkey(windDeBilt, dateTime)
windData <- windDeBilt
```

```{r, HumidityData, cache=TRUE}
humidityDeBilt <- ReadHumidityData("inst/extdata/Sensor/DeBilt_RelativeHumidity.csv")
setkey(humidityDeBilt, dateTime)
```


```{r, dataDeBilt, cache=TRUE, dependson=c('WindData','HumidityData')}
dataDeBilt <- readRDS("results/ResultsDeBilt2015-2016_3hSun.rds")
offsetBeforeSunrise <- 0 #time in minutes
offsetAfterSunset <- 0
#trainDeBilt <- imageSummary[dateTime > sunriseDateTime - offsetBeforeSunrise * 60 & dateTime < sunsetDateTime + offsetAfterSunset * 60, ]
dataDeBilt <- dataDeBilt[dateTime > sunriseDateTime & dateTime < sunsetDateTime, ]
dataDeBilt[, foggy := MOR < 250]
setkey(dataDeBilt, dateTime)
dataDeBilt <- dataDeBilt[windDeBilt, nomatch = 0]
dataDeBilt <- dataDeBilt[humidityDeBilt, nomatch = 0]
dataDeBilt <- na.omit(dataDeBilt)

trainDeBilt <- dataDeBilt[year==2015, .(dateTime, meanEdge, changePoint, smoothness, meanHue,
                meanSaturation, meanBrightness, MOR, foggy, windSpeed, relHumidity) ]

testDeBilt <- dataDeBilt[year==2016, .(dateTime, meanEdge, changePoint, smoothness, meanHue,
                meanSaturation, meanBrightness, MOR, foggy, windSpeed, relHumidity) ]

# features <- c("meanEdge", "changePoint", "smoothness", "meanHue", "meanSaturation", "meanBrightness", "windSpeed")
# 
# 
# StandardizeFeatures <- function(dataSet, features) {
#   dataSet[, (features) := lapply(.SD, function(x) {(x - mean(x))/sd(x) }), .SDcols = features]
#   return(dataSet)
# }
# 
# trainDeBilt <- StandardizeFeatures(trainDeBilt, features)


```

Based on the training set we obtain the following preliminary decision tree:
```{r, Classification, cache = TRUE, include=FALSE, dependson='dataDeBilt'}
fogTree <- rpart(foggy ~ meanEdge + changePoint + meanBrightness + smoothness +
                   meanHue + meanSaturation,
                 trainDeBilt,
                 control = rpart.control(cp = 0.015))
```

```{r, ClassificationTree, echo=FALSE, cache=TRUE, dependson='Classification'}
fancyRpartPlot(fogTree, sub="")
```

The tree has to be read as follows for an image with `meanEdge = 0.0039` and 
`changePoint = 295`. From the top node the lower right node,
with number 3, is reached because `meanEdge >= 0.0041` is false. 
In the next step the lower left node, with number 6, is reached 
because `changePoint < 296` is true. 
This node number 6, tells us that there were 24 instances in the training
set and from these 0 were foggy, i.e. dense fog with `MOR < 250`.
Therefore, the chance that an image in this class is foggy is set to zero.



```{r, echo=F}
evaluatePrediction <- function(t){
cleanedTest <- t[complete.cases(t)] #there are sitations where the MOR is not verified and is given an NA for the measure, thus remove, actually we should remove from the train set and test set before giving to the classifier

predBin <- ifelse(cleanedTest$pred > 0.01, 1, 0) #round(cleanedTest$pred,0)

labels <- cleanedTest$foggy
tp <- sum(predBin == 1 & labels == T)
tp
tn <- sum(predBin == 0 & labels == F)
tn
falsePos <- sum(predBin == 1 & labels ==F)
falsePos
falseNeg <- sum(predBin == 0 & labels ==T)
falseNeg
precision <- tp / (tp + falsePos)
recall <- tp / (tp + falseNeg)
print("precision:")
print(precision)
print("recall:")
print(recall)

print("F1 score:")
print(2*precision*recall/(precision+recall))

# predComp<-prediction(cleanedTest$pred, labels)
# PR.perf <- performance(predComp, "acc")
# plot(PR.perf)
}

```

```{r, EvaluatePrediction, cache=TRUE, dependson=c('dataDeBilt', 'Classification')}
hindVals <- predict(fogTree, trainDeBilt, method = "class")
trainDeBilt[, pred := hindVals]
predVals <- predict(fogTree, testDeBilt, method="class")
testDeBilt[, pred := predVals]

confusionMatrix(ifelse(trainDeBilt$pred > 0.3, TRUE, FALSE), trainDeBilt$foggy)
confusionMatrix(ifelse(testDeBilt$pred > 0.3, TRUE, FALSE), testDeBilt$foggy)

```




Other meteorological parameters are added to the set of features analyzed for the machine learning problem.
```{r, MeteoFeatureAddition, cache=TRUE, dependson='dataDeBilt'}

fogTreeMeteo <- rpart(foggy ~ meanEdge + changePoint + meanBrightness +
                       windSpeed + relHumidity, 
                     trainDeBilt , control = rpart.control(cp = 0.015))
```

```{r, ClassificationTreeWithWind, echo=FALSE, cache=TRUE, dependson='MeteoFeatureAddition'}
fancyRpartPlot(fogTreeMeteo, sub="")
```


```{r, TestWithWind, cache=TRUE, dependson=c('dataDeBilt', 'MeteoFeatureAddition')}
hindVals <- predict(fogTreeMeteo, trainDeBilt, method = "class")
trainDeBilt[, pred := hindVals]
predVals <- predict(fogTreeMeteo, testDeBilt, method="class")
testDeBilt[, pred := predVals]

confusionMatrix(ifelse(trainDeBilt$pred > 0.3, TRUE, FALSE), trainDeBilt$foggy)
confusionMatrix(ifelse(testDeBilt$pred > 0.3, TRUE, FALSE), testDeBilt$foggy)

```

In 2015 we have `r trainDeBilt[foggy == TRUE, .N]` images
in the test set, that are labeled as foggy, i.e. a MOR below 250 meters.

We use the obtained tree to predict the probability of fog from the features of
the images in the test set. In 2016 we have `r testDeBilt[foggy == TRUE, .N]` images
in the test set, that are labeled as foggy, i.e. a MOR below 250 meters. The
table below shows the days, where we either had a high probability of fog
(`> 50%`) and a relatively high MOR (`> 250 meters`) or days with a low 
probability of fog (`< 50%`) and a relatively low MOR (`< 250 meters`).
In total we obtain five days for which this condition is true. 

<!--```{r, ClassificationErrors, echo=FALSE}
knitr::kable(cleanedTest[(pred > 0.5 & MOR > 250) | (pred < 0.5 & MOR < 250),.(dateTime, MOR, meanEdge, changePoint, foggy, pred = round(pred, 2))])
-->

<!--
January 1 2016 is a foggy day. The MOR and the classification mostly give the 
same results. However, at 9:30 and 13:10 the change point detection based on the
transmission estimate leads to wrong a conclusion. 
-->
On January 6 it is already
dark (which should have been filtered out prior to the analysis (time zone
issues)). Moreover, the MOR gives values around 750 meters so the roughly 57% 
probability of fog do not seem too bad. 
On January 23 one is not able to see the radar tower, which is certainly closer
than the reported 1.5 km from the MOR, so here the camera based approach gives
the better result. 

The situation in Twente is different for two reasons, the wide angle of the
camera makes the horizontal averaging of the transmission rate less appropriate.
Moreover, even on a clear day there are only a few edges in the image (which are
mostly very close to the camera, i.e. from the equipment of the automatic 
weather station). Nevertheless, it was quite simple to detect failures of the 
visibility sensor using the two described features, such as during the afternoon
of August 23 2015, where the sensor consistently gave MOR < 250, although the 
image is very clear.

### Regression
When we are interested not only in the distinction between foggy and not foggy,
we can try to build a regression model. In a first exploratory approach we model 
`log(MOR)` as a linear function of `meanEdge + changePoint + meanBrightness`.
```{r, MultiLinearRegression, echo=FALSE, cache=TRUE, dependson='dataDeBilt'}
mlm <- lm(log(MOR) ~ meanEdge + changePoint + meanBrightness, trainDeBilt) 
round(mlm$coefficients, 3)
predVals <- predict(mlm, trainDeBilt)
trainDeBilt[, predMOR := predVals]
```

We can plot the modeled response versus the observations, as seen below:

```{r, UncleanPlot, warning=FALSE, echo=FALSE, fig.height=4, fig.width=4, cache=TRUE, dependson='MultiLinearRegression'}
ggplot(trainDeBilt, aes(x = log(MOR), y = predMOR)) + geom_point() +
  geom_vline(xintercept = c(log(250), log(1000), log(3000), log(5000)), lty = 3) + 
  ylab("modelled log(MOR)")
```

```{r, MultiLinearRegressionWind, echo=FALSE, cache=TRUE, dependson='dataDeBilt'}
mlm <- lm(log(MOR) ~ meanEdge + changePoint + meanBrightness + windSpeed, trainDeBilt) 
round(mlm$coefficients, 3)
predVals <- predict(mlm, trainDeBilt)
trainDeBilt[, predMOR := predVals]
```

We can plot the modeled response versus the observations, as seen below:

```{r, UncleanPlotWind, warning=FALSE, echo=FALSE, fig.height=4, fig.width=4, cache=TRUE, dependson='MultiLinearRegressionWind'}
ggplot(trainDeBilt, aes(x = log(MOR), y = predMOR)) + geom_point() +
  geom_vline(xintercept = c(log(250), log(1000), log(3000), log(5000)), lty = 3) + 
  ylab("modelled log(MOR)")
```

There are some quite apparent features in this plot. For instance the points 
with the very low MOR and a prediction of around 10 are mostly due to an 
failure of the transmissometer. The values with the largest prediction on the
other hand correspond to different sceneries in the picture (we believed the
camera was fixed) or situations where it is already dark. The point on the lower
right corner with a prediction below 7 corresponds again to a different scenery.
```{r, CleanData, echo=FALSE, cache = TRUE, dependson='MultiLinearRegression'}
trainDeBilt[, id := 1:.N]
#
# ggplot(train, aes(x = log(MOR), y = predMOR)) + geom_point()
#
# train[predMOR > 12]
# June 22 2015 11:10 different scenery
# August 4 2015 11:00 different scenery
# September 27 19:10 / 19:20 already dark 
# October 20 18:00 already dark
#
trainDeBilt <- trainDeBilt[predMOR < 12]
#
# ggplot(train, aes(x = log(MOR), y = predMOR)) + geom_point()
#
# train[(log(MOR) < 6 & predMOR > 9)]
# November 1 2015 clear view (afer foogy period / might be time issue)
# November 2 2015 clear view (no fog)
#
trainDeBilt <- trainDeBilt[!(log(MOR) < 6 & predMOR > 9)]
#
# ggplot(train, aes(x = log(MOR), y = predMOR)) + geom_point()
#
# train[(log(MOR) > 8 & predMOR < 9)]
# June 19 2015 9:20 different scenery
# otherwise already dark
#
trainDeBilt <- trainDeBilt[!(log(MOR) > 8 & predMOR < 9)]
#
# ggplot(train, aes(x = log(MOR), y = predMOR)) + geom_point()
#
# train[(log(MOR) < 7 & predMOR > 9.5)]
# June 5 2015 too late
# September 23 too late
# November 2 clear view
# November 26 clear view
#
trainDeBilt <- trainDeBilt[!(log(MOR) < 7 & predMOR > 9.5)]
#
# ggplot(train, aes(x = log(MOR), y = predMOR)) + geom_point()
#
# train[(log(MOR) < 8 & predMOR > 10.2)]
# June too late
# July too late
# November 1 until 16 clear view
# November 1 17:10 already dark
# November 2 29:10 clear view
# November 26 8:40 clear view
#
trainDeBilt <- trainDeBilt[!(log(MOR) < 8 & predMOR > 10.2)]
#
# ggplot(train, aes(x = log(MOR), y = predMOR)) + geom_point()
#
# train[(log(MOR) < 10 & predMOR > 11)]
# June 22 already too late
# October 30 Car in the scenery
#
trainDeBilt <- trainDeBilt[!(log(MOR) < 10 & predMOR > 11)]
#
#
#
```

### Outlook

From the previous section, it is clear that the underlying pictures have to be
reviewed carefully. In the end we want to be able to allow for different views
of the camera, but for this exploratory analysis we have to exclude these data
from the analysis.

Regarding cameras at sites, where no MOR sensor is available regression seems a
big challenge. But the idea of clustering the data, maybe in a semi supervised
way (as there are usually only a few foggy situations), and reporting if the 
probability of fog exceeds a certain level seems relatively close.



### Twente test field

```{r, dataTwente, cache=TRUE}
dataTwente <- readRDS("results/ResultsTwente2015-2016_3hSun.rds")
offsetBeforeSunrise <- 0 #time in minutes
offsetAfterSunset <- 0
#trainDeBilt <- imageSummary[dateTime > sunriseDateTime - offsetBeforeSunrise * 60 & dateTime < sunsetDateTime + offsetAfterSunset * 60, ]
dataTwente <- dataTwente[dateTime > sunriseDateTime & dateTime < sunsetDateTime, ]
dataTwente[, foggy := MOR < 250]
setkey(dataTwente, dateTime)
#dataTwente <- dataTwente[windDeBilt, nomatch = 0]
#dataTwente <- dataTwente[humidityDeBilt, nomatch = 0]
dataTwente <- na.omit(dataTwente)

trainTwente <- dataTwente[year==2015, .(dateTime, meanEdge, changePoint, smoothness, meanHue,
                meanSaturation, meanBrightness, MOR, foggy)] #, windSpeed, relHumidity) ]

testTwente <- dataTwente[year==2016, .(dateTime, meanEdge, changePoint, smoothness, meanHue,
                meanSaturation, meanBrightness, MOR, foggy)] #, windSpeed, relHumidity) ]

```

In 2015 we have `r trainTwente[foggy == TRUE, .N]` images
in the test set, that are labeled as foggy, i.e. a MOR below 250 meters.

Based on the training set we obtain the following preliminary decision tree:
```{r, ClassificationTwente, include=FALSE, cache=TRUE, dependson='dataTwente'}


fogTree <- rpart(foggy ~ meanEdge + changePoint + meanBrightness, trainTwente , control = rpart.control(cp = 0.019))
```

```{r, ClassificationTreeTwente, echo=FALSE, cache=TRUE, dependson='ClassificationTwente'}
fancyRpartPlot(fogTree, sub="")
```

```{r,EvaluatePredictionTwente, cache=TRUE, dependson='ClassificationTwente'}
hindVals <- predict(fogTree, trainTwente, method = "class")
trainTwente[, pred := hindVals]

predVals <- predict(fogTree, testTwente, method="class")
testTwente[, pred := predVals]

confusionMatrix(ifelse(trainTwente$pred > 0.3, TRUE, FALSE), trainTwente$foggy)
confusionMatrix(ifelse(testTwente$pred  > 0.3, TRUE, FALSE), testTwente$foggy)

```


```{r, MultiLinearRegressionTwente, echo=FALSE, cache=TRUE, dependson='dataTwente'}
mlm <- lm(log(MOR) ~ meanEdge + changePoint + meanBrightness, trainTwente) 
round(mlm$coefficients, 3)
predVals <- predict(mlm, trainTwente)
trainTwente[, predMOR := predVals]
```

```{r, UncleanPlotTwente, warning=FALSE, echo=FALSE, fig.height=4, fig.width=4, cache=TRUE, depenson='MultiLinearRegressionTwente'}
ggplot(trainTwente, aes(x = log(MOR), y = predMOR)) + geom_point() +
  geom_vline(xintercept = c(log(250), log(1000), log(3000), log(5000)), lty = 3) + 
  ylab("modelled log(MOR)")
```
